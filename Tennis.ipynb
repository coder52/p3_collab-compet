{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Tennis.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Tennis_Windows_x86_64/Tennis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agents and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agents' performance, if they select actions at random with each time step.  A window should pop up that allows you to observe the agents.\n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agents are able to use their experiences to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 6):                                      # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Score (max over agents) from episode {}: {}'.format(i, np.max(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from ddpg_agent import Agent\n",
    "import torch\n",
    "\n",
    "state_size=env_info.vector_observations[0].shape[0]\n",
    "action_size=brain.vector_action_space_size\n",
    "\n",
    "agent1 = Agent(state_size=state_size, action_size=action_size, random_seed=100)\n",
    "agent2 = Agent(state_size=state_size, action_size=action_size, random_seed=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_agents(agent1, agent2, filename='checkpoint_collab.pth'):\n",
    "    torch.save({\n",
    "        'agent1_actor': agent1.actor_local.state_dict(),\n",
    "        'agent1_critic': agent1.critic_local.state_dict(),\n",
    "        'agent1_actor_optimizer': agent1.actor_optimizer.state_dict(),\n",
    "        'agent1_critic_optimizer': agent1.critic_optimizer.state_dict(),\n",
    "        'agent2_actor': agent2.actor_local.state_dict(),\n",
    "        'agent2_critic': agent2.critic_local.state_dict(),\n",
    "        'agent2_actor_optimizer': agent2.actor_optimizer.state_dict(),\n",
    "        'agent2_critic_optimizer': agent2.critic_optimizer.state_dict(),\n",
    "    }, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.00\n",
      "Episode 200\tAverage Score: 0.00\n",
      "Episode 300\tAverage Score: 0.00\n",
      "Episode 400\tAverage Score: 0.00\n",
      "Episode 500\tAverage Score: 0.00\n",
      "Episode 600\tAverage Score: 0.02\n",
      "Episode 700\tAverage Score: 0.01\n",
      "Episode 800\tAverage Score: 0.01\n",
      "Episode 900\tAverage Score: 0.00\n",
      "Episode 1000\tAverage Score: 0.00\n",
      "Episode 1100\tAverage Score: 0.00\n",
      "Episode 1200\tAverage Score: 0.00\n",
      "Episode 1300\tAverage Score: 0.01\n",
      "Episode 1400\tAverage Score: 0.02\n",
      "Episode 1500\tAverage Score: 0.01\n",
      "Episode 1600\tAverage Score: 0.02\n",
      "Episode 1700\tAverage Score: 0.04\n",
      "Episode 1800\tAverage Score: 0.02\n",
      "Episode 1900\tAverage Score: 0.02\n",
      "Episode 2000\tAverage Score: 0.02\n",
      "Episode 2100\tAverage Score: 0.05\n",
      "Episode 2200\tAverage Score: 0.08\n",
      "Episode 2300\tAverage Score: 0.03\n",
      "Episode 2400\tAverage Score: 0.19\n",
      "Episode 2500\tAverage Score: 0.44\n",
      "Episode 2600\tAverage Score: 0.19\n",
      "Episode 2700\tAverage Score: 0.38\n",
      "Episode 2800\tAverage Score: 0.34\n",
      "Episode 2900\tAverage Score: 0.41\n",
      "Episode 2948\tAverage Score: 0.50\n",
      "Environment solved in 2948 episodes!\tAverage Score: 0.50\n",
      "Episode 2949\tAverage Score: 0.51\n",
      "Environment solved in 2949 episodes!\tAverage Score: 0.51\n",
      "Episode 2951\tAverage Score: 0.52\n",
      "Environment solved in 2951 episodes!\tAverage Score: 0.52\n",
      "Episode 2952\tAverage Score: 0.52\n",
      "Environment solved in 2952 episodes!\tAverage Score: 0.52\n",
      "Episode 2953\tAverage Score: 0.52\n",
      "Environment solved in 2953 episodes!\tAverage Score: 0.52\n",
      "Episode 2954\tAverage Score: 0.53\n",
      "Environment solved in 2954 episodes!\tAverage Score: 0.53\n",
      "Episode 2955\tAverage Score: 0.53\n",
      "Environment solved in 2955 episodes!\tAverage Score: 0.53\n",
      "Episode 2960\tAverage Score: 0.56\n",
      "Environment solved in 2960 episodes!\tAverage Score: 0.56\n",
      "Episode 2961\tAverage Score: 0.57\n",
      "Environment solved in 2961 episodes!\tAverage Score: 0.57\n",
      "Episode 2962\tAverage Score: 0.60\n",
      "Environment solved in 2962 episodes!\tAverage Score: 0.60\n",
      "Episode 2963\tAverage Score: 0.60\n",
      "Environment solved in 2963 episodes!\tAverage Score: 0.60\n",
      "Episode 2964\tAverage Score: 0.63\n",
      "Environment solved in 2964 episodes!\tAverage Score: 0.63\n",
      "Episode 2965\tAverage Score: 0.67\n",
      "Environment solved in 2965 episodes!\tAverage Score: 0.67\n",
      "Episode 2967\tAverage Score: 0.69\n",
      "Environment solved in 2967 episodes!\tAverage Score: 0.69\n",
      "Episode 2968\tAverage Score: 0.72\n",
      "Environment solved in 2968 episodes!\tAverage Score: 0.72\n",
      "Episode 2970\tAverage Score: 0.72\n",
      "Environment solved in 2970 episodes!\tAverage Score: 0.72\n",
      "Episode 2971\tAverage Score: 0.75\n",
      "Environment solved in 2971 episodes!\tAverage Score: 0.75\n",
      "Episode 2972\tAverage Score: 0.77\n",
      "Environment solved in 2972 episodes!\tAverage Score: 0.77\n",
      "Episode 2973\tAverage Score: 0.82\n",
      "Environment solved in 2973 episodes!\tAverage Score: 0.82\n",
      "Episode 2974\tAverage Score: 0.85\n",
      "Environment solved in 2974 episodes!\tAverage Score: 0.85\n",
      "Episode 2975\tAverage Score: 0.87\n",
      "Environment solved in 2975 episodes!\tAverage Score: 0.87\n",
      "Episode 2976\tAverage Score: 0.88\n",
      "Environment solved in 2976 episodes!\tAverage Score: 0.88\n",
      "Episode 2977\tAverage Score: 0.89\n",
      "Environment solved in 2977 episodes!\tAverage Score: 0.89\n",
      "Episode 2978\tAverage Score: 0.89\n",
      "Environment solved in 2978 episodes!\tAverage Score: 0.89\n",
      "Episode 2982\tAverage Score: 0.90\n",
      "Environment solved in 2982 episodes!\tAverage Score: 0.90\n",
      "Episode 2985\tAverage Score: 0.91\n",
      "Environment solved in 2985 episodes!\tAverage Score: 0.91\n",
      "Episode 2987\tAverage Score: 0.92\n",
      "Environment solved in 2987 episodes!\tAverage Score: 0.92\n",
      "Episode 2994\tAverage Score: 0.94\n",
      "Environment solved in 2994 episodes!\tAverage Score: 0.94\n",
      "Episode 2995\tAverage Score: 0.96\n",
      "Environment solved in 2995 episodes!\tAverage Score: 0.96\n",
      "Episode 2997\tAverage Score: 0.96\n",
      "Environment solved in 2997 episodes!\tAverage Score: 0.96\n",
      "Episode 3000\tAverage Score: 0.96\n",
      "Episode 3006\tAverage Score: 0.97\n",
      "Environment solved in 3006 episodes!\tAverage Score: 0.97\n",
      "Episode 3010\tAverage Score: 0.97\n",
      "Environment solved in 3010 episodes!\tAverage Score: 0.97\n",
      "Episode 3011\tAverage Score: 0.97\n",
      "Environment solved in 3011 episodes!\tAverage Score: 0.97\n",
      "Episode 3012\tAverage Score: 0.97\n",
      "Environment solved in 3012 episodes!\tAverage Score: 0.97\n",
      "Episode 3013\tAverage Score: 0.99\n",
      "Environment solved in 3013 episodes!\tAverage Score: 0.99\n",
      "Episode 3020\tAverage Score: 1.02\n",
      "Environment solved in 3020 episodes!\tAverage Score: 1.02\n",
      "Episode 3021\tAverage Score: 1.02\n",
      "Environment solved in 3021 episodes!\tAverage Score: 1.02\n",
      "Episode 3022\tAverage Score: 1.03\n",
      "Environment solved in 3022 episodes!\tAverage Score: 1.03\n",
      "Episode 3023\tAverage Score: 1.04\n",
      "Environment solved in 3023 episodes!\tAverage Score: 1.04\n",
      "Episode 3024\tAverage Score: 1.06\n",
      "Environment solved in 3024 episodes!\tAverage Score: 1.06\n",
      "Episode 3026\tAverage Score: 1.07\n",
      "Environment solved in 3026 episodes!\tAverage Score: 1.07\n",
      "Episode 3027\tAverage Score: 1.11\n",
      "Environment solved in 3027 episodes!\tAverage Score: 1.11\n",
      "Episode 3028\tAverage Score: 1.16\n",
      "Environment solved in 3028 episodes!\tAverage Score: 1.16\n",
      "Episode 3029\tAverage Score: 1.16\n",
      "Environment solved in 3029 episodes!\tAverage Score: 1.16\n",
      "Episode 3030\tAverage Score: 1.17\n",
      "Environment solved in 3030 episodes!\tAverage Score: 1.17\n",
      "Episode 3031\tAverage Score: 1.18\n",
      "Environment solved in 3031 episodes!\tAverage Score: 1.18\n",
      "Episode 3032\tAverage Score: 1.20\n",
      "Environment solved in 3032 episodes!\tAverage Score: 1.20\n",
      "Episode 3033\tAverage Score: 1.21\n",
      "Environment solved in 3033 episodes!\tAverage Score: 1.21\n",
      "Episode 3034\tAverage Score: 1.23\n",
      "Environment solved in 3034 episodes!\tAverage Score: 1.23\n",
      "Episode 3035\tAverage Score: 1.24\n",
      "Environment solved in 3035 episodes!\tAverage Score: 1.24\n",
      "Episode 3038\tAverage Score: 1.25\n",
      "Environment solved in 3038 episodes!\tAverage Score: 1.25\n",
      "Episode 3039\tAverage Score: 1.27\n",
      "Environment solved in 3039 episodes!\tAverage Score: 1.27\n",
      "Episode 3042\tAverage Score: 1.27\n",
      "Environment solved in 3042 episodes!\tAverage Score: 1.27\n",
      "Episode 3043\tAverage Score: 1.28\n",
      "Environment solved in 3043 episodes!\tAverage Score: 1.28\n",
      "Episode 3044\tAverage Score: 1.29\n",
      "Environment solved in 3044 episodes!\tAverage Score: 1.29\n",
      "Episode 3045\tAverage Score: 1.30\n",
      "Environment solved in 3045 episodes!\tAverage Score: 1.30\n",
      "Episode 3047\tAverage Score: 1.31\n",
      "Environment solved in 3047 episodes!\tAverage Score: 1.31\n",
      "Episode 3049\tAverage Score: 1.31\n",
      "Environment solved in 3049 episodes!\tAverage Score: 1.31\n",
      "Episode 3053\tAverage Score: 1.31\n",
      "Environment solved in 3053 episodes!\tAverage Score: 1.31\n",
      "Episode 3055\tAverage Score: 1.33\n",
      "Environment solved in 3055 episodes!\tAverage Score: 1.33\n",
      "Episode 3056\tAverage Score: 1.33\n",
      "Environment solved in 3056 episodes!\tAverage Score: 1.33\n",
      "Episode 3057\tAverage Score: 1.33\n",
      "Environment solved in 3057 episodes!\tAverage Score: 1.33\n",
      "Episode 3100\tAverage Score: 1.23\n",
      "Episode 3200\tAverage Score: 1.10\n",
      "Episode 3300\tAverage Score: 0.72\n",
      "Episode 3400\tAverage Score: 0.84\n",
      "Episode 3500\tAverage Score: 0.98\n",
      "Episode 3600\tAverage Score: 1.05\n",
      "Episode 3700\tAverage Score: 0.79\n",
      "Episode 3750\tAverage Score: 1.33\n",
      "Environment solved in 3750 episodes!\tAverage Score: 1.33\n",
      "Episode 3751\tAverage Score: 1.35\n",
      "Environment solved in 3751 episodes!\tAverage Score: 1.35\n",
      "Episode 3755\tAverage Score: 1.35\n",
      "Environment solved in 3755 episodes!\tAverage Score: 1.35\n",
      "Episode 3767\tAverage Score: 1.35\n",
      "Environment solved in 3767 episodes!\tAverage Score: 1.35\n",
      "Episode 3769\tAverage Score: 1.36\n",
      "Environment solved in 3769 episodes!\tAverage Score: 1.36\n",
      "Episode 3770\tAverage Score: 1.36\n",
      "Environment solved in 3770 episodes!\tAverage Score: 1.36\n",
      "Episode 3771\tAverage Score: 1.37\n",
      "Environment solved in 3771 episodes!\tAverage Score: 1.37\n",
      "Episode 3772\tAverage Score: 1.38\n",
      "Environment solved in 3772 episodes!\tAverage Score: 1.38\n",
      "Episode 3776\tAverage Score: 1.40\n",
      "Environment solved in 3776 episodes!\tAverage Score: 1.40\n",
      "Episode 3778\tAverage Score: 1.41\n",
      "Environment solved in 3778 episodes!\tAverage Score: 1.41\n",
      "Episode 3779\tAverage Score: 1.45\n",
      "Environment solved in 3779 episodes!\tAverage Score: 1.45\n",
      "Episode 3780\tAverage Score: 1.49\n",
      "Environment solved in 3780 episodes!\tAverage Score: 1.49\n",
      "Episode 3782\tAverage Score: 1.53\n",
      "Environment solved in 3782 episodes!\tAverage Score: 1.53\n",
      "Episode 3783\tAverage Score: 1.58\n",
      "Environment solved in 3783 episodes!\tAverage Score: 1.58\n",
      "Episode 3784\tAverage Score: 1.63\n",
      "Environment solved in 3784 episodes!\tAverage Score: 1.63\n",
      "Episode 3785\tAverage Score: 1.67\n",
      "Environment solved in 3785 episodes!\tAverage Score: 1.67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3786\tAverage Score: 1.72\n",
      "Environment solved in 3786 episodes!\tAverage Score: 1.72\n",
      "Episode 3789\tAverage Score: 1.76\n",
      "Environment solved in 3789 episodes!\tAverage Score: 1.76\n",
      "Episode 3791\tAverage Score: 1.80\n",
      "Environment solved in 3791 episodes!\tAverage Score: 1.80\n",
      "Episode 3792\tAverage Score: 1.82\n",
      "Environment solved in 3792 episodes!\tAverage Score: 1.82\n",
      "Episode 3794\tAverage Score: 1.84\n",
      "Environment solved in 3794 episodes!\tAverage Score: 1.84\n",
      "Episode 3795\tAverage Score: 1.88\n",
      "Environment solved in 3795 episodes!\tAverage Score: 1.88\n",
      "Episode 3796\tAverage Score: 1.91\n",
      "Environment solved in 3796 episodes!\tAverage Score: 1.91\n",
      "Episode 3797\tAverage Score: 1.93\n",
      "Environment solved in 3797 episodes!\tAverage Score: 1.93\n",
      "Episode 3798\tAverage Score: 1.93\n",
      "Environment solved in 3798 episodes!\tAverage Score: 1.93\n",
      "Episode 3799\tAverage Score: 1.97\n",
      "Environment solved in 3799 episodes!\tAverage Score: 1.97\n",
      "Episode 3800\tAverage Score: 2.00\n",
      "\n",
      "Environment solved in 3800 episodes!\tAverage Score: 2.00\n",
      "Episode 3801\tAverage Score: 2.02\n",
      "Environment solved in 3801 episodes!\tAverage Score: 2.02\n",
      "Episode 3802\tAverage Score: 2.05\n",
      "Environment solved in 3802 episodes!\tAverage Score: 2.05\n",
      "Episode 3803\tAverage Score: 2.08\n",
      "Environment solved in 3803 episodes!\tAverage Score: 2.08\n",
      "Episode 3804\tAverage Score: 2.13\n",
      "Environment solved in 3804 episodes!\tAverage Score: 2.13\n",
      "Episode 3805\tAverage Score: 2.13\n",
      "Environment solved in 3805 episodes!\tAverage Score: 2.13\n",
      "Episode 3808\tAverage Score: 2.17\n",
      "Environment solved in 3808 episodes!\tAverage Score: 2.17\n",
      "Episode 3811\tAverage Score: 2.20\n",
      "Environment solved in 3811 episodes!\tAverage Score: 2.20\n",
      "Episode 3812\tAverage Score: 2.21\n",
      "Environment solved in 3812 episodes!\tAverage Score: 2.21\n",
      "Episode 3813\tAverage Score: 2.26\n",
      "Environment solved in 3813 episodes!\tAverage Score: 2.26\n",
      "Episode 3815\tAverage Score: 2.28\n",
      "Environment solved in 3815 episodes!\tAverage Score: 2.28\n",
      "Episode 3816\tAverage Score: 2.32\n",
      "Environment solved in 3816 episodes!\tAverage Score: 2.32\n",
      "Episode 3817\tAverage Score: 2.37\n",
      "Environment solved in 3817 episodes!\tAverage Score: 2.37\n",
      "Episode 3818\tAverage Score: 2.40\n",
      "Environment solved in 3818 episodes!\tAverage Score: 2.40\n",
      "Episode 3900\tAverage Score: 1.50\n",
      "Episode 4000\tAverage Score: 2.38\n",
      "Episode 4001\tAverage Score: 2.43\n",
      "Environment solved in 4001 episodes!\tAverage Score: 2.43\n",
      "Episode 4002\tAverage Score: 2.46\n",
      "Environment solved in 4002 episodes!\tAverage Score: 2.46\n",
      "Episode 4003\tAverage Score: 2.46\n",
      "Environment solved in 4003 episodes!\tAverage Score: 2.46\n",
      "Episode 4004\tAverage Score: 2.49\n",
      "Environment solved in 4004 episodes!\tAverage Score: 2.49\n",
      "Episode 4005\tAverage Score: 2.55\n",
      "Environment solved in 4005 episodes!\tAverage Score: 2.55\n",
      "Episode 4006\tAverage Score: 2.60\n",
      "Environment solved in 4006 episodes!\tAverage Score: 2.60\n",
      "Episode 4007\tAverage Score: 2.65\n",
      "Environment solved in 4007 episodes!\tAverage Score: 2.65\n",
      "Episode 4010\tAverage Score: 2.66\n",
      "Environment solved in 4010 episodes!\tAverage Score: 2.66\n",
      "Episode 4014\tAverage Score: 2.67\n",
      "Environment solved in 4014 episodes!\tAverage Score: 2.67\n",
      "Episode 4016\tAverage Score: 2.67\n",
      "Environment solved in 4016 episodes!\tAverage Score: 2.67\n",
      "Episode 4017\tAverage Score: 2.71\n",
      "Environment solved in 4017 episodes!\tAverage Score: 2.71\n",
      "Episode 4018\tAverage Score: 2.73\n",
      "Environment solved in 4018 episodes!\tAverage Score: 2.73\n",
      "Episode 4019\tAverage Score: 2.73\n",
      "Environment solved in 4019 episodes!\tAverage Score: 2.73\n",
      "Episode 4041\tAverage Score: 2.76\n",
      "Environment solved in 4041 episodes!\tAverage Score: 2.76\n",
      "Episode 4042\tAverage Score: 2.78\n",
      "Environment solved in 4042 episodes!\tAverage Score: 2.78\n",
      "Episode 4047\tAverage Score: 2.82\n",
      "Environment solved in 4047 episodes!\tAverage Score: 2.82\n",
      "Episode 4048\tAverage Score: 2.87\n",
      "Environment solved in 4048 episodes!\tAverage Score: 2.87\n",
      "Episode 4049\tAverage Score: 2.90\n",
      "Environment solved in 4049 episodes!\tAverage Score: 2.90\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-a5cae7452211>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0magent1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mactions2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0magent2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mactions1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[0magent2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mactions2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\udacity_RL_projects\\p3_collab-compet\\ddpg_agent.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[0mexperiences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_noise\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\udacity_RL_projects\\p3_collab-compet\\ddpg_agent.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, experiences, gamma)\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[0mcritic_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;31m# ---------------------------- update actor ---------------------------- #\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\p2_continous-control\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\p2_continous-control\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\p2_continous-control\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    142\u001b[0m                    \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'weight_decay'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m                    eps=group['eps'])\n\u001b[0m\u001b[0;32m    145\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\p2_continous-control\\lib\\site-packages\\torch\\optim\\_functional.py\u001b[0m in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[1;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0mexp_avg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m             \u001b[1;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_episodes = 5000\n",
    "max_t = 1000\n",
    "print_every = 100\n",
    "target_average_score = 0.5\n",
    "\n",
    "\n",
    "scores_deque = deque(maxlen=100)\n",
    "scores_list = []\n",
    "average_scores = []\n",
    "noise = False\n",
    "for i_episode in range(1, n_episodes+1):\n",
    "    # Suppress noise for the first episodes, thus fill the buffer with more random actions.\n",
    "    if i_episode>200:\n",
    "        noise=True\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    states = env_info.vector_observations\n",
    "    agent1.reset()\n",
    "    agent2.reset()\n",
    "    scores = np.zeros(num_agents)\n",
    "    \n",
    "    for t in range(max_t):\n",
    "        actions1 = agent1.act(states[0], add_noise=noise)\n",
    "        actions2 = agent2.act(states[1], add_noise=noise)\n",
    "        env_info = env.step(np.stack((actions1, actions2)))[brain_name]\n",
    "        next_states = env_info.vector_observations\n",
    "        rewards = env_info.rewards                         \n",
    "        dones = env_info.local_done            \n",
    "        \n",
    "        # slow down the actions by multiplying them by a coefficient.\n",
    "        agent1.step(states[0], 0.5 * actions1, rewards[0], next_states[0], dones[0])\n",
    "        agent1.step(states[1], 0.5 * actions2, rewards[1], next_states[1], dones[1])\n",
    "        \n",
    "        agent2.step(states[0], 0.5 * actions1, rewards[0], next_states[0], dones[0])\n",
    "        agent2.step(states[1], 0.5 * actions2, rewards[1], next_states[1], dones[1])\n",
    "\n",
    "        scores += max(env_info.rewards)                         \n",
    "        states = next_states  \n",
    "        if np.any(dones):\n",
    "            break\n",
    "\n",
    "    scores_deque.append(np.mean(scores))\n",
    "    scores_list.append(scores)      \n",
    "\n",
    "    # Sigma decay\n",
    "    sigma_decay = 0.995\n",
    "    min_sigma = 0.05\n",
    "    if agent1.noise.sigma > min_sigma:\n",
    "        agent1.noise.decay_sigma(sigma_decay)\n",
    "\n",
    "    if agent2.noise.sigma > min_sigma:\n",
    "        agent2.noise.decay_sigma(sigma_decay)\n",
    "\n",
    "    average_score = np.mean(scores_deque)\n",
    "    average_scores.append(average_score)\n",
    "    print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, average_score), end=\"\")\n",
    "    if i_episode % print_every == 0:\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, average_score))  \n",
    "    if average_score > target_average_score:        \n",
    "        print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, average_score))\n",
    "        save_agents(agent1, agent2, filename='checkpoint_collab.pth')\n",
    "        \n",
    "        target_average_score = average_score\n",
    "#         break; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores_list)+1), scores_list, color=\"grey\")\n",
    "plt.plot(np.arange(1, len(scores_list)+1), average_scores, color=\"red\")\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load saved models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "checkpoint = torch.load('checkpoint_collab.pth')\n",
    "\n",
    "agent1.actor_local.load_state_dict(checkpoint['agent1_actor'])\n",
    "agent1.critic_local.load_state_dict(checkpoint['agent1_critic'])\n",
    "agent1.actor_optimizer.load_state_dict(checkpoint['agent1_actor_optimizer'])\n",
    "agent1.critic_optimizer.load_state_dict(checkpoint['agent1_critic_optimizer'])\n",
    "\n",
    "agent2.actor_local.load_state_dict(checkpoint['agent2_actor'])\n",
    "agent2.critic_local.load_state_dict(checkpoint['agent2_critic'])\n",
    "agent2.actor_optimizer.load_state_dict(checkpoint['agent2_actor_optimizer'])\n",
    "agent2.critic_optimizer.load_state_dict(checkpoint['agent2_critic_optimizer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score (max over agents) from episode 1: 2.3000000342726707\n",
      "Score (max over agents) from episode 2: 2.2000000327825546\n",
      "Score (max over agents) from episode 3: 2.3000000342726707\n",
      "Score (max over agents) from episode 4: 2.2000000327825546\n",
      "Score (max over agents) from episode 5: 2.500000037252903\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):                                      # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        actions1 = agent1.act(states[0], add_noise=False)\n",
    "        actions2 = agent2.act(states[1], add_noise=False)\n",
    "        env_info = env.step(np.stack((actions1, actions2)))[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Score (max over agents) from episode {}: {}'.format(i, np.max(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
