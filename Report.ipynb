{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15f7666b",
   "metadata": {},
   "source": [
    "# Udacity P3: Collab Compet Project Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cbf4db",
   "metadata": {},
   "source": [
    "## 1. Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6f6a8c",
   "metadata": {},
   "source": [
    "This project implements a **Multi-Agent Deep Deterministic Policy Gradient (MADDPG)** approach to solve the **Tennis** environment from Unity ML-Agents.\n",
    "The goal is for two agents to keep a ball in play by hitting it back and forth, maximizing their average score over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79875161",
   "metadata": {},
   "source": [
    "## 2. Environment Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0622a5b8",
   "metadata": {},
   "source": [
    "\n",
    "- **State size:** 24 (per agent)\n",
    "\n",
    "- **Action size:** 2 (racket movement: horizontal and vertical)\n",
    "\n",
    "- **Number of agents:** 2\n",
    "\n",
    "- **Reward function:**\n",
    "\n",
    "    - **+0.1** for hitting the ball over the net\n",
    "\n",
    "    - **-0.01** if the ball hits the ground"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce18eaa0",
   "metadata": {},
   "source": [
    "## 3. Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f2a9c4",
   "metadata": {},
   "source": [
    "### 3.1 Learning Algorithm\n",
    "\n",
    "In this project, the **Deep Deterministic Policy Gradient (DDPG)** algorithm was implemented to train the agent.\n",
    "DDPG is an off-policy actor-critic algorithm designed for continuous action spaces. It consists of:\n",
    "\n",
    "* **Actor network:** Proposes continuous actions given a state.\n",
    "\n",
    "* **Critic network:** Estimates the Q-value of state-action pairs to evaluate the actor’s actions.\n",
    "\n",
    "* **Replay buffer:** Stores past experiences to stabilize training.\n",
    "\n",
    "* **Soft target updates:** Gradual update of target networks.\n",
    "\n",
    "* **Exploration noise:** Ornstein-Uhlenbeck process to encourage exploration.\n",
    "\n",
    "**Note:** Adaptive Noise Decay: To encourage exploration during the early stages of training and gradually shift toward exploitation (σ *= 0.995) at each step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488abfea",
   "metadata": {},
   "source": [
    "### 3.2 Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9e2dea",
   "metadata": {},
   "source": [
    "* **Actor network:** 3 fully connected layers with 400, 300 neurons, ReLU activations; output layer uses tanh activation to keep actions within valid range.\n",
    "\n",
    "* **Critic network:** 3 fully connected layers; the first layer processes state inputs, followed by concatenation with actions and further layers with ReLU activations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4d52da",
   "metadata": {},
   "source": [
    "### 3.3 Hyperparameters\n",
    "\n",
    "    BATCH_SIZE = 128        # Number of experiences sampled per training step to update the networks.\n",
    "    GAMMA = 0.99            # Discount factor determining how much future rewards are taken into account.\n",
    "    TAU = 1e-3              # Rate at which target networks softly track the learned networks.\n",
    "    BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "    LR_ACTOR = 1e-4         # learning rate of the actor\n",
    "    LR_CRITIC = 1e-3        # learning rate of the critic\n",
    "    WEIGHT_DECAY = 0        # L2 regularization term to prevent overfitting, set to zero here.\n",
    "    sigma_decay = 0.995     # Factor by which exploration noise decreases gradually over time.\n",
    "    min_sigma = 0.05        # Minimum allowed value for noise level to ensure some exploration remains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3fd4e8",
   "metadata": {},
   "source": [
    "## 4. Plot of Rewards According to Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f565723",
   "metadata": {},
   "source": [
    "Environment solved in 2948 episodes!\tAverage Score: 0.50\n",
    "\n",
    "It was observed that the average reward always increased in subsequent epochs. As agents learned, the duration of each episode increased, so training was manually stopped after 4000 epochs.\n",
    "  \n",
    " <img src=\"img/img.png\" style=\"float: left;\"/>\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad00d57",
   "metadata": {},
   "source": [
    "## 5. Results\n",
    "\n",
    "* Initially, the agents acted almost randomly due to lack of experience and sparse rewards.\n",
    "\n",
    "* Over time, they began to coordinate better, keeping the ball in play longer.\n",
    "\n",
    "* The MADDPG approach proved effective for continuous control in a cooperative–competitive multi-agent setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd5a89e",
   "metadata": {},
   "source": [
    "## 6. Ideas for Future Work\n",
    "\n",
    "* Experiment with reward shaping (e.g., bonus for faster ball speed) to speed up learning.\n",
    "\n",
    "* Hyperparameter tuning (learning rates, τ, γ) for faster convergence.\n",
    "\n",
    "* Try a double-critic architecture (TD3-style) for more stable learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2db53b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
